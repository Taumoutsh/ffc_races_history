services:
  # Main application service
  race-cycling-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: race-cycling-app
    restart: unless-stopped
    ports:
      - "5000:5000"
    volumes:
      # Mount database directory for external scraping access
      - ./data:/app/data
      # Mount logs directory
      - ./logs:/app/logs
      # Optional: Mount additional scripts or configurations
      - ./backend/scrapers:/app/backend/scrapers:ro
      # Share frontend files with nginx
      - frontend_dist:/app/frontend/dist
    environment:
      - FLASK_ENV=production
      - DB_PATH=/app/data/cycling_data.db
      - AUTH_DB_PATH=/app/data/auth.db
      - PORT=5000
      - HOST=0.0.0.0
    networks:
      - race-cycling-net
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:5000/api/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Reverse proxy with static file serving
  nginx:
    image: nginx:alpine
    container_name: race-cycling-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
      - ./logs:/var/log/nginx
      - frontend_dist:/usr/share/nginx/html:ro
    networks:
      - race-cycling-net

  # Optional: Database scraper service (can be run separately)
  scraper:
    build:
      context: .
      dockerfile: Dockerfile.scraper
    container_name: race-cycling-scraper
    restart: "no"
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - DB_PATH=/app/data/cycling_data.db
      - SCRAPER_LOG_LEVEL=INFO
    networks:
      - race-cycling-net
    profiles:
      - scraper
    depends_on:
      - race-cycling-app

networks:
  race-cycling-net:
    driver: bridge

volumes:
  # Named volumes for persistence (optional)
  race_data:
    driver: local
  race_logs:
    driver: local
  frontend_dist:
    driver: local